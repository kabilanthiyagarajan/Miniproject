## Real-Time Emotion Detection Using Deep Learning
Real-Time Emotion Detection Using Deep Learning is a project focused on developing a sophisticated system that identifies and classifies human emotions through facial expressions, voice, and body language. Traditional emotion detection methods often require extensive manual intervention and are limited in scope. This project aims to enhance the user experience by providing an automated, efficient solution for recognizing emotions in real time. The system leverages advanced machine learning techniques, including Convolutional Neural Networks (CNN) and Multi-task Cascaded Convolutional Networks (MTCNN), to accurately analyze visual and auditory data.

## Features
1.Utilizes advanced CNN and MTCNN algorithms for robust emotion recognition.
2.Real-time processing for immediate feedback and interaction.
3.Supports multiple input sources, including video streams and audio files.
4.High accuracy in emotion classification through comprehensive training datasets.
5.User-friendly interface for seamless interaction and engagement.
## Requirements
1.Operating System: Requires a 64-bit OS (Windows 10 or Ubuntu) to ensure compatibility with deep learning frameworks.
2.Development Environment: Python 3.6 or later is essential for implementing the emotion detection model.
3.Deep Learning Frameworks: TensorFlow is used for model training and implementation.
4.Image Processing Libraries: OpenCV is crucial for real-time video capture and image processing.
5.IDE: Use of PyCharm or VSCode as the Integrated Development Environment for coding, debugging, and version control integration.
6.Additional Dependencies: Includes TensorFlow, OpenCV, NumPy, and other libraries necessary for handling deep learning tasks and data processing.

## System Architecture
<!--Embed the system architecture diagram as shown below-->

![Screenshot 2023-11-25 133637](https://github.com/<<yourusername>>/Hand-Gesture-Recognition-System/assets/75235455/a60c11f3-0a11-47fb-ac89-755d5f45c995)


## Output

#### Output1 - Emotion1
![Screenshot 2024-10-30 111032](https://github.com/user-attachments/assets/1d4be640-6f0e-44c3-911b-aefbe8c8243b)


#### Output2 - Emotion2
![Screenshot 2024-10-30 111133](https://github.com/user-attachments/assets/cdccc9cf-ba50-43dc-84ca-bd9383ed4a15)


## Results and Impact
<!--Give the results and impact as shown below-->
The Sign Language Detection System enhances accessibility for individuals with hearing and speech impairments, providing a valuable tool for inclusive communication. The project's integration of computer vision and deep learning showcases its potential for intuitive and interactive human-computer interaction.

This project serves as a foundation for future developments in assistive technologies and contributes to creating a more inclusive and accessible digital environment.

## Articles published / References
1.Wang, S., Li, B. Z., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-Attention with Linear Complexity. In Proceedings of the 37th International Conference on Machine Learning (ICML).Link: https://arxiv.org/abs/2006.04768

2.Ahn, H., Ha, T., Choi, Y., Yoo, H., & Oh, S. (2018). Text2Action: Generative Adversarial Synthesis from Language to Action. In Proceedings of the International Conference on Robotics and Automation (ICRA).Link: https://arxiv.org/abs/1804.03530

3.Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer Normalization. arXiv preprint arXiv:1607.06450.Link: https://arxiv.org/abs/1607.06450

4.Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. arXiv preprint arXiv:1409.0473.Link: https://arxiv.org/abs/1409.047

5.Saunders, B., Camgoz, N. C., & Bowden, R. (2021). Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks. International Journal of Computer Vision, 1â€“23.Link: https://arxiv.org/abs/2104.05347

6.Mollahosseini, A., Chan, D., & Mahoor, M. H. (2017). Going Deeper in Facial Expression Recognition Using Deep Neural Networks. Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW).Link: https://arxiv.org/abs/1708.00825

7.Zadeh, A., Chen, M., Poria, S., & Morency, L. P. (2018). Multimodal Emotion Recognition Using Audio, Visual and Textual Cues. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
Link: https://ieeexplore.ieee.org/document/8462642
8. Khalid, S., Shah, A., & Chen, S. (2020). Emotion Recognition from Facial Expressions using Deep Learning: A Comprehensive Review. Artificial Intelligence Review, 53(3), 1439-1455.
https://link.springer.com/article/10.1007/s10462-019-09731-y
9. Ghimire, D., & Hsu, H. (2021). Real-time Facial Emotion Recognition Using Deep Learning and OpenCV. 2021 IEEE International Conference on Smart Computing (SMARTCOMP), 1-8.
https://ieeexplore.ieee.org/document/9472807
10.  Krause, M. R., Kaseb, A., & Tang, Y. (2018). Facial Emotion Recognition: A Comparative Study of Deep Learning Models. Journal of Computer Vision and Image Understanding, 172, 12-20.
https://www.sciencedirect.com/science/article/pii/S1077314218300179




